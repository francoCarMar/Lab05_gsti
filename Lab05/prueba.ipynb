{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc9f804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas.\n",
      "\n",
      "--- 1. Cargando y Explorando los Datos ---\n",
      "Archivo 'data/water_potability.csv' cargado exitosamente.\n",
      "\n",
      "--- Información General (Tipos de Dato y Conteo) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               2785 non-null   float64\n",
      " 1   Hardness         3276 non-null   float64\n",
      " 2   Solids           3276 non-null   float64\n",
      " 3   Chloramines      3276 non-null   float64\n",
      " 4   Sulfate          2495 non-null   float64\n",
      " 5   Conductivity     3276 non-null   float64\n",
      " 6   Organic_carbon   3276 non-null   float64\n",
      " 7   Trihalomethanes  3114 non-null   float64\n",
      " 8   Turbidity        3276 non-null   float64\n",
      " 9   Potability       3276 non-null   int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 256.1 KB\n",
      "\n",
      "--- Conteo de Valores Nulos (Missing) ---\n",
      "ph                 491\n",
      "Hardness             0\n",
      "Solids               0\n",
      "Chloramines          0\n",
      "Sulfate            781\n",
      "Conductivity         0\n",
      "Organic_carbon       0\n",
      "Trihalomethanes    162\n",
      "Turbidity            0\n",
      "Potability           0\n",
      "dtype: int64\n",
      "\n",
      "--- Desbalance de Clases (Objetivo 'Potability') ---\n",
      "Potability\n",
      "0    0.60989\n",
      "1    0.39011\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- 2. Implementando Selección de Características (Método Embebido) ---\n",
      "Preparando datos para el modelo selector...\n",
      "Datos divididos: 2620 para entrenamiento, 656 para prueba.\n",
      "Creando pipeline de preprocesamiento (Impute -> Transform/Standardize)...\n",
      "Ajustando pipeline de preprocesamiento SÓLO en datos de train...\n",
      "Aplicando balanceo de clases (SMOTE) al set de entrenamiento...\n",
      "Tamaño de Train antes de SMOTE: 2620\n",
      "Tamaño de Train después de SMOTE: 3172\n",
      "\n",
      "Entrenando modelo selector para obtener importancias...\n",
      "\n",
      "--- Resultados de Importancia de Características (Método Embebido) ---\n",
      "           Feature  Importance\n",
      "0               ph   28.662655\n",
      "4          Sulfate   28.215883\n",
      "3      Chloramines   14.285104\n",
      "2           Solids   11.819918\n",
      "1         Hardness    6.861411\n",
      "7  Trihalomethanes    3.621181\n",
      "8        Turbidity    3.002740\n",
      "6   Organic_carbon    2.048497\n",
      "5     Conductivity    1.482610\n",
      "\n",
      "ACCIÓN: Se seleccionan las 5 mejores características: ['ph', 'Sulfate', 'Chloramines', 'Solids', 'Hardness', 'Trihalomethanes']\n",
      "\n",
      "--- 3. Optimizando y Entrenando Modelo FINAL (sobre 5 características) ---\n",
      "Iniciando GridSearchCV (esto puede tardar)...\n",
      "Mejores Parámetros encontrados: {'depth': 8, 'iterations': 1000, 'learning_rate': 0.03}\n",
      "Mejor Score F1 (CV): 0.6968\n",
      "\n",
      "--- Evaluación del Modelo FINAL Optimizado ---\n",
      "Precisión (Accuracy) - Modelo Final: 63.72%\n",
      "\n",
      "Reporte de Clasificación - Modelo Final:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71       412\n",
      "           1       0.51      0.54      0.52       244\n",
      "\n",
      "    accuracy                           0.64       656\n",
      "   macro avg       0.61      0.62      0.62       656\n",
      "weighted avg       0.64      0.64      0.64       656\n",
      "\n",
      "\n",
      "Matriz de Confusión - Modelo FINAL:\n",
      "[[287 125]\n",
      " [113 131]]\n"
     ]
    }
   ],
   "source": [
    "# --- PASO 0: Importar Librerías ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# --- Librerías para Preprocesamiento Avanzado ---\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer # Para Transformación (normal) y Estandarización\n",
    "from sklearn.pipeline import Pipeline # Usaremos pipelines para simplificar\n",
    "from imblearn.over_sampling import SMOTE # Para balanceo avanzado\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Configuración\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "sns.set(style=\"darkgrid\")\n",
    "print(\"Librerías importadas.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# --- PASO 1: Entender las columnas del archivo CSV ---\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n--- 1. Cargando y Explorando los Datos ---\")\n",
    "try:\n",
    "    df = pd.read_csv('data/water_potability.csv')\n",
    "    print(\"Archivo 'data/water_potability.csv' cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'data/water_potability.csv' no encontrado.\")\n",
    "    print(\"Asegúrate de que el archivo esté en la ruta correcta antes de continuar.\")\n",
    "    raise SystemExit(\"Deteniendo ejecución: archivo no encontrado.\")\n",
    "\n",
    "print(\"\\n--- Información General (Tipos de Dato y Conteo) ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- Conteo de Valores Nulos (Missing) ---\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"\\n--- Desbalance de Clases (Objetivo 'Potability') ---\")\n",
    "print(df['Potability'].value_counts(normalize=True))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# --- PASO 2: Selección de Características (Método Embebido) ---\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n--- 2. Implementando Selección de Características (Método Embebido) ---\")\n",
    "# Usaremos CatBoost (un método embebido) para obtener la importancia de características.\n",
    "# Para hacerlo correctamente, primero debemos preprocesar los datos.\n",
    "\n",
    "# 2a. Preparar datos para el modelo selector\n",
    "print(\"Preparando datos para el modelo selector...\")\n",
    "X = df.drop('Potability', axis=1)\n",
    "y = df['Potability']\n",
    "feature_names = X.columns # Guardar nombres para después\n",
    "\n",
    "# 2b. Dividir en Train y Test (ANTES de cualquier preprocesamiento)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Datos divididos: {len(X_train)} para entrenamiento, {len(X_test)} para prueba.\")\n",
    "\n",
    "# 2c. Pipeline de Preprocesamiento Avanzado\n",
    "# 1. KNNImputer (Imputación avanzada)\n",
    "# 2. PowerTransformer (Transformación a distribución normal y Estandarización)\n",
    "print(\"Creando pipeline de preprocesamiento (Impute -> Transform/Standardize)...\")\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('transformer', PowerTransformer(method='yeo-johnson', standardize=True)) # Estandariza (Normaliza)\n",
    "])\n",
    "\n",
    "# Ajustar (fit) el pipeline SÓLO con datos de train (evita data leakage)\n",
    "print(\"Ajustando pipeline de preprocesamiento SÓLO en datos de train...\")\n",
    "X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
    "# Transformar el set de prueba\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
    "\n",
    "# Convertir de nuevo a DataFrames\n",
    "X_train_processed = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=feature_names)\n",
    "print(\"Datos de entrenamiento y prueba preprocesados.\")\n",
    "\n",
    "# 2d. Balanceo de Clases Avanzado (SMOTE)\n",
    "# Se aplica SÓLO al set de entrenamiento\n",
    "print(\"Aplicando balanceo de clases (SMOTE) al set de entrenamiento...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train)\n",
    "print(f\"Tamaño de Train antes de SMOTE: {X_train_processed.shape[0]}\")\n",
    "print(f\"Tamaño de Train después de SMOTE: {X_train_balanced.shape[0]}\")\n",
    "\n",
    "# 2e. Entrenar modelo \"Selector\" para obtener importancias\n",
    "# (Este modelo se entrena con los datos preprocesados, balanceados y TODAS las características)\n",
    "print(\"\\nEntrenando modelo selector para obtener importancias...\")\n",
    "selector_model = CatBoostClassifier(\n",
    "    iterations=500, learning_rate=0.05, eval_metric='Accuracy',\n",
    "    random_seed=42, verbose=0,\n",
    "    allow_writing_files=False # Evitar archivos de log\n",
    ")\n",
    "\n",
    "# Usamos los datos balanceados para el entrenamiento del selector\n",
    "selector_model.fit(\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    eval_set=(X_test_processed, y_test),\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# 2f. Obtener y mostrar resultados de importancia\n",
    "print(\"\\n--- Resultados de Importancia de Características ---\")\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': selector_model.get_feature_importance()\n",
    "})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(importance_df.to_string())\n",
    "\n",
    "# 2g. Decisión: Nos quedamos con las 5 características principales\n",
    "# (Basado en la imagen 'image_fb9c0a.png', las 5 primeras tienen > 10% de importancia)\n",
    "features_to_keep = importance_df.head(5)['Feature'].tolist()\n",
    "print(f\"\\nACCIÓN: Se seleccionan las 5 mejores características: {features_to_keep}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# --- PASO 3: Optimización y Entrenamiento del Modelo FINAL ---\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n--- 3. Optimizando y Entrenando Modelo FINAL (sobre 5 características) ---\")\n",
    "\n",
    "# 3a. Reducir los DataFrames a las características seleccionadas\n",
    "X_train_final = X_train_balanced[features_to_keep]\n",
    "X_test_final = X_test_processed[features_to_keep] # El Test set NO se balancea\n",
    "\n",
    "# 3b. Definir los parámetros a probar (GridSearchCV)\n",
    "param_grid = {\n",
    "    'depth': [4, 6, 8],\n",
    "    'learning_rate': [0.03, 0.05, 0.1],\n",
    "    'iterations': [500, 1000]\n",
    "}\n",
    "\n",
    "# 3c. Inicializar el modelo base para el GridSearch\n",
    "# No usamos 'scale_pos_weight' porque ya balanceamos con SMOTE\n",
    "grid_search_model = CatBoostClassifier(\n",
    "    random_seed=42,\n",
    "    eval_metric='F1', # Optimizamos para F1, mejor métrica en desbalance\n",
    "    verbose=0,\n",
    "    allow_writing_files=False\n",
    ")\n",
    "\n",
    "# 3d. Configurar y ejecutar GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=grid_search_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3, # 3-Fold Cross-Validation\n",
    "    scoring='f1_weighted', # Usamos F1 ponderado para la búsqueda\n",
    "    n_jobs=-1 # Usar todos los núcleos de CPU\n",
    ")\n",
    "\n",
    "print(\"Iniciando GridSearchCV (esto puede tardar)...\")\n",
    "# Entrenamos la búsqueda sobre los datos de entrenamiento balanceados y reducidos\n",
    "grid_search.fit(X_train_final, y_train_balanced)\n",
    "\n",
    "print(f\"Mejores Parámetros encontrados: {grid_search.best_params_}\")\n",
    "print(f\"Mejor Score F1 (CV): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 3e. Obtener el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 3f. Evaluación e Interpretación del Modelo Final\n",
    "print(\"\\n--- Evaluación del Modelo FINAL Optimizado ---\")\n",
    "# Predecimos sobre el X_test_final (reducido, pero no balanceado)\n",
    "y_pred_final = best_model.predict(X_test_final)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_final)\n",
    "print(f\"Precisión (Accuracy) - Modelo Final: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nReporte de Clasificación - Modelo Final:\")\n",
    "# Reportamos contra el y_test original\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "\n",
    "# Imprimir la Matriz de Confusión en texto\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(\"\\nMatriz de Confusión - Modelo FINAL:\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
